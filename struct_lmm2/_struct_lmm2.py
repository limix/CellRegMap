from glimix_core.lmm import LMM
from numpy import concatenate, diag, empty, inf, ones, sqrt, stack, trace
from numpy.linalg import eigvalsh, inv, solve
from numpy_sugar import ddot
from numpy_sugar.linalg import economic_qs

from ._math import (
    P_matrix,
    qmin,
    score_statistic,
    score_statistic_distr_weights,
    score_statistic_liu_params,
)


class StructLMM2:
    r"""
    Mixed-model with genetic effect heterogeneity.

    The extended StructLMM model (two random effects) is:

        ğ² = Wğ›‚ + ğ âŠ™ğ›ƒ + ğ + ğ® + ğ›†,                                               (1)

    where:

        ğ›ƒ âˆ¼ ğ“(ğŸ, ğ“‹â‚€((1-Ïâ‚€)ğŸğŸáµ€ + Ïâ‚€ğ™´ğ™´áµ€)),
        ğ âˆ¼ ğ“(ğŸ, ğ“‹â‚Ïâ‚EEáµ€),
        ğ® âˆ¼ ğ“(ğŸ, ğ“‹â‚(1-Ïâ‚)ğ™º), and
        ğ›† âˆ¼ ğ“(ğŸ, ğ“‹â‚‚ğ™¸).

    ğ âŠ™ğ›ƒ is made of two components: the persistent genotype effect and the GxE effect.
    ğ is the environment effect, ğ® is the population structure effect, and ğ›† is the iid
    noise. The full covariance of ğ² is therefore given by:

        cov(ğ²) = ğ“‹â‚€(1-Ïâ‚€)ğŸğŸáµ€ + ğ“‹â‚€Ïâ‚€ğ™´ğ™´áµ€ + ğ“‹â‚Ïâ‚EEáµ€ + ğ“‹â‚(1-Ïâ‚)ğ™º + ğ“‹â‚‚ğ™¸.

    Its marginalised form is given by:

        ğ² âˆ¼ ğ“(Wğ›‚, ğ“‹â‚€ğ™³((1-Ïâ‚€)ğŸğŸáµ€ + Ïâ‚€ğ™´ğ™´áµ€)ğ™³ + ğ“‹â‚(Ïâ‚EEáµ€ + (1-Ïâ‚)ğ™º) + ğ“‹â‚‚ğ™¸),

    where ğ™³ = diag(ğ ).

    StructLMM method is used to perform two types of statistical tests.

    1. The association test compares the following hypotheses (from Eq.1):

        ğ“—â‚€: ğ“‹â‚€ = 0
        ğ“—â‚: ğ“‹â‚€ > 0

    ğ“—â‚€ denotes no genetic association, while ğ“—â‚ models any genetic association.
    In particular, ğ“—â‚ includes genotype-environment interaction as part of genetic
    association.

    2. The interaction test is slighlty different as the persistent genotype
    effect is now considered to be a fixed effect, and added to the model as an
    additional covariate term:

        ğ² = Wğ›‚ + ğ ğ›½â‚ + ğ âŠ™ğ›ƒâ‚‚ + ğ + ğ® + ğ›†,                                        (2)

    where:

        ğ›ƒâ‚‚ âˆ¼ ğ“(ğŸ, ğ“‹â‚ƒğ™´ğ™´áµ€),
        ğ âˆ¼ ğ“(ğŸ, ğ“‹â‚Ïâ‚ğ™´ğ™´áµ€),
        ğ® ~ ğ“(ğŸ, ğ“‹â‚(1-Ïâ‚)ğ™º), and
        ğ›† âˆ¼ ğ“(ğŸ, ğ“‹â‚‚ğ™¸).

    We refer to this modified model as the interaction model.
    The compared hypotheses are:

        ğ“—â‚€: ğ“‹â‚ƒ = 0
        ğ“—â‚: ğ“‹â‚ƒ > 0
    """

    def __init__(self, y, W, E, K=None):
        self._y = y
        self._W = W
        self._E = E
        self._K = K
        self._EE = E @ E.T

        self._rho0 = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
        self._rho1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
        self._null_lmm_assoc = {}

        self._Sigma = {}
        self._Sigma_qs = {}
        for rho1 in self._rho1:
            # Î£ = Ïâ‚ğ™´ğ™´áµ€ + (1-Ïâ‚)ğ™º
            self._Sigma[rho1] = rho1 * self._EE + (1 - rho1) * self._K
            self._Sigma_qs[rho1] = economic_qs(self._Sigma[rho1])

    def fit_null_association(self):
        best = {"lml": -inf, "lmm": None, "rho1": -1.0}
        for rho1, Sigma_qs in self._Sigma_qs.items():
            lmm = LMM(self._y, self._W, Sigma_qs, restricted=True)
            lmm.fit(verbose=True)
            lml = lmm.lml()
            if lml > best["lml"]:
                best["lml"] = lml
                best["lmm"] = lmm
                best["rho1"] = rho1
        self._null_lmm_assoc = best

    def scan_association(self, G):
        n_snps = G.shape[1]
        lmm = self._null_lmm_assoc["lmm"]
        K0 = lmm.covariance()
        P = P_matrix(self._W, K0)
        # H1 vs H0 via score test
        for i in range(n_snps):
            g = G[:, i].reshape(G.shape[0], 1)
            K0 = lmm.covariance()
            weights = []
            liu_params = []
            for rho0 in self._rho0:
                # K = K0 + s2total * (
                #     (1 - rho0) * g @ g.T + rho0 * diag(g) @ self._EE @ diag(g)
                # )
                dK = (1 - rho0) * g @ g.T + rho0 * diag(g) @ self._EE @ diag(g)
                # ğ™¿ = ğ™ºâ»Â¹ - ğ™ºâ»Â¹ğš†(ğš†áµ€ğ™ºâ»Â¹ğš†)â»Â¹ğš†áµ€ğ™ºâ»Â¹
                # ğ‘„ = Â½ğ²áµ€ğ™¿(âˆ‚ğ™º)ğ™¿ğ².
                Q = score_statistic(self._y, self._W, K0, dK)
                weights += [score_statistic_distr_weights(self._W, K0, dK)]
                liu_params += [score_statistic_liu_params(Q, weights)]

            q = qmin(liu_params)

            # 3. Calculate quantities that occur in null distribution
            # g has to be a column-vector
            D = diag(g.ravel())
            Pg = P @ g
            m = (g.T @ Pg)[0, 0]
            M = 1 / m * (sqrtm(P) @ g @ g.T @ sqrtm(P))
            H1 = E.T @ D.T @ P @ D @ E
            H2 = E.T @ D.T @ sqrtm(P) @ M @ sqrtm(P) @ D @ E
            H = H1 - H2
            lambdas = eigvalsh(H / 2)
            lambdas = eigh

            eta = ETxPx11xPxE @ ZTIminusMZ
            vareta = 4 * trace(eta)

            OneZTZE = 0.5 * (g.T @ PxoE)
            tau_top = OneZTZE @ OneZTZE.T
            tau_rho = empty(len(self._rho0))
            for i in range(len(self._rho0)):
                tau_rho[i] = self._rho0[i] * m + (1 - self._rho0[i]) / m * tau_top

            MuQ = sum(eigh)
            VarQ = sum(eigh ** 2) * 2 + vareta
            KerQ = sum(eigh ** 4) / (sum(eigh ** 2) ** 2) * 12
            Df = 12 / KerQ


#     def _score_stats_null_dist(self, g):
#         """
#         Under the null hypothesis, the score-based test statistic follows a weighted sum
#         of random variables:
#             ğ‘„ âˆ¼ âˆ‘áµ¢ğœ†áµ¢Ï‡Â²(1),
#         where ğœ†áµ¢ are the non-zero eigenvalues of Â½âˆšğ™¿(âˆ‚ğ™º)âˆšğ™¿.
#         Note that
#             âˆ‚ğ™ºáµ¨ = ğ™³(ÏğŸğŸáµ€ + (1-Ï)ğ™´ğ™´áµ€)ğ™³ = (Ïğ ğ áµ€ + (1-Ï)ğ™´Ìƒğ™´Ìƒáµ€)
#         for ğ™´Ìƒ = ğ™³ğ™´.
#         By using SVD decomposition, one can show that the non-zero eigenvalues of ğš‡ğš‡áµ€
#         are equal to the non-zero eigenvalues of ğš‡áµ€ğš‡.
#         Therefore, ğœ†áµ¢ are the non-zero eigenvalues of
#             Â½[âˆšÏğ  âˆš(1-Ï)ğ™´Ìƒ]ğ™¿[âˆšÏğ  âˆš(1-Ï)ğ™´Ìƒ]áµ€.
#         """
#         # Pâ‚€ğ² = Kâ»Â¹ğ² - Kâ‚€â»Â¹X(Xáµ€Kâ‚€â»Â¹X)â»Â¹Xáµ€Kâ‚€â»Â¹ğ².

#         K0 = self._null_lmm_assoc["lmm"].covariance()
#         K0iy = solve(K0, self._y)
#         X = self._W
#         P0y = K0iy - solve(K0, X @ solve(X.T @ K0iX, X.T @ K0iy))

#         # The covariance matrix of H1 is K = Kâ‚€ + bÂ²diag(ğ )â‹…Î£â‹…diag(ğ )
#         # We have âˆ‚K/âˆ‚bÂ² = diag(ğ )â‹…Î£â‹…diag(ğ )
#         # The score test statistics is given by
#         # Q = Â½ğ²áµ€Pâ‚€â‹…âˆ‚Kâ‹…Pâ‚€ğ²
#         n_samples = len(g)
#         dK_G = ddot(g.ravel(), ddot(ones((n_samples, n_samples)), g.ravel()))
#         dK_GxE = ddot(g.ravel(), ddot(self._EE, g.ravel()))

#         # P0 = P0 + 1e-9 * eye(P0.shape[0])
#         Q_G = P0y.T @ dK_G @ P0y
#         Q_GxE = P0y.T @ dK_GxE @ P0y

#         P0 = inv(K0) - K0iX @ solve(X.T @ K0iX, K0iX.T)
#         # the eigenvalues of Â½Pâ‚€â‹…âˆ‚Kâ‹…Pâ‚€
#         # are tge eigenvalues of
#         gPg = g.T @ P0 @ g
#         goE = g * self._E
#         gPgoE = g.T @ P0 @ goE
#         gEPgE = goE.T @ P0 @ goE

#         lambdas = []
#         Q = []
#         for rho0 in self._rho0:
#             Q.append((rho0 * Q_G + (1 - rho0) * Q_GxE) / 2)
#             F[0, 0] = rho0 * gPg
#             F[0, 1:] = sqrt(rho0) * sqrt(1 - rho0) * gPgoE
#             F[1:, 0] = F[0, 1:]
#             F[1:, 1:] = (1 - rho0) * gEPgE
#             lambdas.append(eigvalsh(F) / 2)

#         return lambdas


# def _score_stats_null_dist(g):
#     """
#     Under the null hypothesis, the score-based test statistic follows a weighted sum
#     of random variables:
#         ğ‘„ âˆ¼ âˆ‘áµ¢ğœ†áµ¢Ï‡Â²(1),
#     where ğœ†áµ¢ are the non-zero eigenvalues of Â½âˆšğ™¿(âˆ‚ğ™º)âˆšğ™¿.
#     Note that
#         âˆ‚ğ™ºáµ¨ = ğ™³(ÏğŸğŸáµ€ + (1-Ï)ğ™´ğ™´áµ€)ğ™³ = (Ïğ ğ áµ€ + (1-Ï)ğ™´Ìƒğ™´Ìƒáµ€)
#     for ğ™´Ìƒ = ğ™³ğ™´.
#     By using SVD decomposition, one can show that the non-zero eigenvalues of ğš‡ğš‡áµ€
#     are equal to the non-zero eigenvalues of ğš‡áµ€ğš‡.
#     Therefore, ğœ†áµ¢ are the non-zero eigenvalues of
#         Â½[âˆšÏğ  âˆš(1-Ï)ğ™´Ìƒ]ğ™¿[âˆšÏğ  âˆš(1-Ï)ğ™´Ìƒ]áµ€.
#     """
#     # Pâ‚€ğ² = Kâ»Â¹ğ² - Kâ‚€â»Â¹X(Xáµ€Kâ‚€â»Â¹X)â»Â¹Xáµ€Kâ‚€â»Â¹ğ².
#     K0 = self._null_lmm_assoc["lmm"].covariance()
#     K0iy = solve(K0, self._y)
#     X = self._W
#     K0iX = solve(K0, X)
#     P0y = K0iy - solve(K0, X @ solve(X.T @ K0iX, X.T @ K0iy))

#     # The covariance matrix of H1 is K = Kâ‚€ + bÂ²diag(ğ )â‹…Î£â‹…diag(ğ )
#     # We have âˆ‚K/âˆ‚bÂ² = diag(ğ )â‹…Î£â‹…diag(ğ )
#     # The score test statistics is given by
#     # Q = Â½ğ²áµ€Pâ‚€â‹…âˆ‚Kâ‹…Pâ‚€ğ²
#     n_samples = len(g)
#     dK_G = ddot(g.ravel(), ddot(ones((n_samples, n_samples)), g.ravel()))
#     dK_GxE = ddot(g.ravel(), ddot(self._EE, g.ravel()))

#     # P0 = P0 + 1e-9 * eye(P0.shape[0])
#     Q_G = P0y.T @ dK_G @ P0y
#     Q_GxE = P0y.T @ dK_GxE @ P0y

#     P0 = inv(K0) - K0iX @ solve(X.T @ K0iX, K0iX.T)
#     # the eigenvalues of Â½Pâ‚€â‹…âˆ‚Kâ‹…Pâ‚€
#     # are tge eigenvalues of
#     gPg = g.T @ P0 @ g
#     goE = g * self._E
#     gPgoE = g.T @ P0 @ goE
#     gEPgE = goE.T @ P0 @ goE

#     lambdas = []
#     Q = []
#     for rho0 in self._rho0:
#         Q.append((rho0 * Q_G + (1 - rho0) * Q_GxE) / 2)
#         F[0, 0] = rho0 * gPg
#         F[0, 1:] = sqrt(rho0) * sqrt(1 - rho0) * gPgoE
#         F[1:, 0] = F[0, 1:]
#         F[1:, 1:] = (1 - rho0) * gEPgE
#         lambdas.append(eigvalsh(F) / 2)

#     return lambdas
