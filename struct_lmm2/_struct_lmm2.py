from glimix_core.lmm import LMM
from numpy import asarray, concatenate, diag, empty, inf, ones, sqrt, stack, trace
from numpy.linalg import eigvalsh, inv, solve
from numpy_sugar import ddot
from numpy_sugar.linalg import economic_qs
from scipy.linalg import sqrtm

from ._math import (
    P_matrix,
    qmin,
    score_statistic,
    score_statistic_distr_weights,
    score_statistic_liu_params,
)


class StructLMM2:
    r"""
    Mixed-model with genetic effect heterogeneity.

    The extended StructLMM model (two random effects) is:

        ğ² = Wğ›‚ + ğ âŠ™ğ›ƒ + ğ + ğ® + ğ›†,                                               (1)

    where:

        ğ›ƒ âˆ¼ ğ“(ğŸ, ğ“‹â‚€((1-Ïâ‚€)ğŸğŸáµ€ + Ïâ‚€ğ™´ğ™´áµ€)),
        ğ âˆ¼ ğ“(ğŸ, ğ“‹â‚Ïâ‚EEáµ€),
        ğ® âˆ¼ ğ“(ğŸ, ğ“‹â‚(1-Ïâ‚)ğ™º), and
        ğ›† âˆ¼ ğ“(ğŸ, ğ“‹â‚‚ğ™¸).

    ğ âŠ™ğ›ƒ is made of two components: the persistent genotype effect and the GxE effect.
    ğ is the environment effect, ğ® is the population structure effect, and ğ›† is the iid
    noise. The full covariance of ğ² is therefore given by:

        cov(ğ²) = ğ“‹â‚€(1-Ïâ‚€)ğ™³ğŸğŸáµ€ğ™³ + ğ“‹â‚€Ïâ‚€ğ™³ğ™´ğ™´áµ€ğ™³ + ğ“‹â‚Ïâ‚EEáµ€ + ğ“‹â‚(1-Ïâ‚)ğ™º + ğ“‹â‚‚ğ™¸.

    Its marginalised form is given by:

        ğ² âˆ¼ ğ“(Wğ›‚, ğ“‹â‚€ğ™³((1-Ïâ‚€)ğŸğŸáµ€ + Ïâ‚€ğ™´ğ™´áµ€)ğ™³ + ğ“‹â‚(Ïâ‚EEáµ€ + (1-Ïâ‚)ğ™º) + ğ“‹â‚‚ğ™¸),

    where ğ™³ = diag(ğ ).

    StructLMM method is used to perform two types of statistical tests.

    1. The association test compares the following hypotheses (from Eq.1):

        ğ“—â‚€: ğ“‹â‚€ = 0
        ğ“—â‚: ğ“‹â‚€ > 0

    ğ“—â‚€ denotes no genetic association, while ğ“—â‚ models any genetic association.
    In particular, ğ“—â‚ includes genotype-environment interaction as part of genetic
    association.

    2. The interaction test is slighlty different as the persistent genotype
    effect is now considered to be a fixed effect, and added to the model as an
    additional covariate term:

        ğ² = Wğ›‚ + ğ ğ›½â‚ + ğ âŠ™ğ›ƒâ‚‚ + ğ + ğ® + ğ›†,                                        (2)

    where:

        ğ›ƒâ‚‚ âˆ¼ ğ“(ğŸ, ğ“‹â‚ƒğ™´ğ™´áµ€),
        ğ âˆ¼ ğ“(ğŸ, ğ“‹â‚Ïâ‚ğ™´ğ™´áµ€),
        ğ® ~ ğ“(ğŸ, ğ“‹â‚(1-Ïâ‚)ğ™º), and
        ğ›† âˆ¼ ğ“(ğŸ, ğ“‹â‚‚ğ™¸).

    We refer to this modified model as the interaction model.
    The compared hypotheses are:

        ğ“—â‚€: ğ“‹â‚ƒ = 0
        ğ“—â‚: ğ“‹â‚ƒ > 0
    """

    def __init__(self, y, W, E, K=None):
        self._y = y
        self._W = W
        self._E = E
        self._K = K
        self._EE = E @ E.T

        self._rho0 = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
        self._rho1 = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
        self._null_lmm_assoc = {}

        self._Sigma = {}
        self._Sigma_qs = {}
        for rho1 in self._rho1:
            # Î£ = Ïâ‚ğ™´ğ™´áµ€ + (1-Ïâ‚)ğ™º
            self._Sigma[rho1] = rho1 * self._EE + (1 - rho1) * self._K
            self._Sigma_qs[rho1] = economic_qs(self._Sigma[rho1])

    def fit_null_association(self):
        best = {"lml": -inf, "lmm": None, "rho1": -1.0}
        for rho1, Sigma_qs in self._Sigma_qs.items():
            lmm = LMM(self._y, self._W, Sigma_qs, restricted=True)
            lmm.fit(verbose=True)
            lml = lmm.lml()
            if lml > best["lml"]:
                best["lml"] = lml
                best["lmm"] = lmm
                best["rho1"] = rho1
        self._null_lmm_assoc = best

    def scan_association(self, G):
        n_snps = G.shape[1]
        lmm = self._null_lmm_assoc["lmm"]
        K0 = lmm.covariance()
        P = P_matrix(self._W, K0)
        # H1 vs H0 via score test
        for i in range(n_snps):
            g = G[:, i].reshape(G.shape[0], 1)
            K0 = lmm.covariance()
            weights = []
            liu_params = []
            for rho0 in self._rho0:
                # K = K0 + s2total * (
                #     (1 - rho0) * g @ g.T + rho0 * diag(g) @ self._EE @ diag(g)
                # )
                dK = (1 - rho0) * g @ g.T + rho0 * diag(g) @ self._EE @ diag(g)
                # ğ™¿ = ğ™ºâ»Â¹ - ğ™ºâ»Â¹ğš†(ğš†áµ€ğ™ºâ»Â¹ğš†)â»Â¹ğš†áµ€ğ™ºâ»Â¹
                # ğ‘„ = Â½ğ²áµ€ğ™¿(âˆ‚ğ™º)ğ™¿ğ².
                Q = score_statistic(self._y, self._W, K0, dK)
                weights += [score_statistic_distr_weights(self._W, K0, dK)]
                liu_params += [score_statistic_liu_params(Q, weights)]

            q = qmin(liu_params)

            # 3. Calculate quantities that occur in null distribution
            # g has to be a column-vector
            D = diag(g.ravel())
            Pg = P @ g
            m = (g.T @ Pg)[0, 0]
            M = 1 / m * (sqrtm(P) @ g @ g.T @ sqrtm(P))
            H1 = E.T @ D.T @ P @ D @ E
            H2 = E.T @ D.T @ sqrtm(P) @ M @ sqrtm(P) @ D @ E
            H = H1 - H2
            lambdas = eigvalsh(H / 2)
            lambdas = eigh

            eta = ETxPx11xPxE @ ZTIminusMZ
            vareta = 4 * trace(eta)

            OneZTZE = 0.5 * (g.T @ PxoE)
            tau_top = OneZTZE @ OneZTZE.T
            tau_rho = empty(len(self._rho0))
            for i in range(len(self._rho0)):
                tau_rho[i] = self._rho0[i] * m + (1 - self._rho0[i]) / m * tau_top

            MuQ = sum(eigh)
            VarQ = sum(eigh ** 2) * 2 + vareta
            KerQ = sum(eigh ** 4) / (sum(eigh ** 2) ** 2) * 12
            Df = 12 / KerQ

    def scan_interaction(self, G):
        from chiscore import davies_pvalue

        n_snps = G.shape[1]
        pvalues = []
        for i in range(n_snps):
            g = G[:, [i]]
            Wg = concatenate((self._W, g), axis=1)
            best = {"lml": -inf, "a": 0, "v0": 0, "v1": 0, "beta": 0}
            for a in self._rho1:
                QS = self._Sigma_qs[a]
                # cov(y) = v0*(aÎ£ + (1-a)K) + v1*Is
                lmm = LMM(self._y, Wg, QS, restricted=True)
                lmm.fit(verbose=False)
                if lmm.lml() > best["lml"]:
                    best["lml"] = lmm.lml()
                    best["a"] = a
                    best["v0"] = lmm.v0
                    best["v1"] = lmm.v1
                    best["alpha"] = lmm.beta
                    best["lmm"] = lmm

            lmm = best["lmm"]
            "H1 via score test"
            # Let Kâ‚€ = gÂ²K + eÂ²Î£ + ğœ€Â²I
            # with optimal values eÂ² and ğœ€Â² found above.
            K0 = lmm.covariance()
            X = concatenate((self._E, g), axis=1)

            # Let Pâ‚€ = Kâ»Â¹ - Kâ‚€â»Â¹X(Xáµ€Kâ‚€â»Â¹X)â»Â¹Xáµ€Kâ‚€â»Â¹.
            K0iX = solve(K0, X)
            P0 = inv(K0) - K0iX @ solve(X.T @ K0iX, K0iX.T)

            # Pâ‚€ğ² = Kâ»Â¹ğ² - Kâ‚€â»Â¹X(Xáµ€Kâ‚€â»Â¹X)â»Â¹Xáµ€Kâ‚€â»Â¹ğ².
            K0iy = solve(K0, self._y)
            P0y = K0iy - solve(K0, X @ solve(X.T @ K0iX, X.T @ K0iy))

            # The covariance matrix of H1 is K = Kâ‚€ + bÂ²diag(ğ )â‹…Î£â‹…diag(ğ )
            # We have âˆ‚K/âˆ‚bÂ² = diag(ğ )â‹…Î£â‹…diag(ğ )
            # The score test statistics is given by
            # Q = Â½ğ²áµ€Pâ‚€â‹…âˆ‚Kâ‹…Pâ‚€ğ²
            dK = ddot(g.ravel(), ddot(self._EE, g.ravel()))
            Q = (P0y.T @ dK @ P0y) / 2

            # Q is the score statistic for our interaction test and follows a linear combination
            # of chi-squared (df=1) distributions:
            # Q âˆ¼ âˆ‘Î»Ï‡Â², where Î»áµ¢ are the non-zero eigenvalues of Â½âˆšPâ‚€â‹…âˆ‚Kâ‹…âˆšPâ‚€.
            sqrP0 = sqrtm(P0)
            pval = davies_pvalue(Q, (sqrP0 @ dK @ sqrP0) / 2)
            pvalues.append(pval)

        return asarray(pvalues, float)
