
import numpy as np
import scipy as sp
from numpy import concatenate, inf, newaxis
from numpy.linalg import eigvalsh, inv, solve
from numpy.random import RandomState
from numpy_sugar import epsilon
from numpy_sugar.linalg import ddot, economic_qs, economic_svd
from scipy.linalg import sqrtm
from chiscore import davies_pvalue, mod_liu, optimal_davies_pvalue

from glimix_core.lmm import LMM


class StructLMM2:
    r'''
    Mixed-model with genetic effect heterogeneity.

    The extended StructLMM model (two random effects) is ::

        ð² = Wð›‚ + ð ð›½ + ð âŠ™ð›ƒ + ðž + u + ð›†,

    where ::

        ð âŠ™ð›ƒ = âˆ‘áµ¢ð áµ¢ð›½áµ¢
        ð›ƒ âˆ¼ ð“(ðŸŽ, bÂ²Î£)
        ðž âˆ¼ ð“(ðŸŽ, eÂ²Î£)
        ð›† âˆ¼ ð“(ðŸŽ, ðœ€Â²I)
        Î£ = EEáµ€
        u ~ ð“(ðŸŽ, gÂ²K)

    If one considers ð›½ âˆ¼ ð“(0, pÂ²), we can insert
    ð›½ into ð›ƒ ::

        ð›ƒ_ âˆ¼ ð“(ðŸŽ, pÂ²ðŸðŸáµ€ + bÂ²Î£)
    '''
    '''
    test
    random = RandomState(0)
    n = 1000
    c = 2
    y = random.randn(n)
    W = random.randn(n, c)
    # g = random.randn(n)
    E = random.randn(n, 4)
    Sigma = E @ E.T
    X = random.randn(n, 5)
    K = X @ X.T
    '''

    def __init__(self, y, W, E, G = None, a_values = None, K = None):

        self.y = y
        self.E = E
        self.G = G
        self.W = W

        self.Sigma = E @ E.T

        if self.G is None:
            self.K = np.eye(self.y.shape[0])
        else:
            self.K = G @ G.T

        self.a_values = a_values
        if self.a_values is None:
            self.a_values = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
        self.Cov = {}
        self.QS_a = {}
        for a in self.a_values:
            self.Cov[a] = a * self.Sigma + (1 - a) * self.K
            self.QS_a[a] = economic_qs(self.Cov[a])


    def fit_null(self, g):
        self.X = concatenate((self.W, g[:, newaxis]), axis = 1)
        best = {"lml": -inf, "a": 0, "v0": 0, "v1": 0, "beta": 0}
        for a in self.a_values:
        # cov(y) = v0*(aÎ£ + (1-a)K) + v1*I
            lmm = LMM(self.y, self.X, self.QS_a[a], restricted = True)
            lmm.fit(verbose = False)
            if lmm.lml() > best["lml"]:
                best["lml"] = lmm.lml()
                best["a"] = a
                best["v0"] = lmm.v0
                best["v1"] = lmm.v1
                best["alpha"] = lmm.beta
                best["covariance"] = lmm.covariance()
        self.best = best


    def score_2_dof(self, g):
        alpha = self.best["alpha"][:-1]
        beta = self.best["alpha"][-1]
        # eÂ²Î£ + gÂ²K = sÂ²(aÎ£ + (1-a)K)
        # eÂ² = sÂ²*a
        # gÂ² = sÂ²*(1-a)
        s2 = self.best["v0"] # sÂ²
        eps2 = self.best["v1"]  # ðœ€Â²

        # H1 via score test
        # Let Kâ‚€ = gÂ²K + eÂ²Î£ + ðœ€Â²I
        # with optimal values eÂ² and ðœ€Â² found above.
        K0 = self.best["covariance"]

        # Let Pâ‚€ = Kâ»Â¹ - Kâ‚€â»Â¹X(Xáµ€Kâ‚€â»Â¹X)â»Â¹Xáµ€Kâ‚€â»Â¹.
        K0iX = solve(K0, self.X)
        P0 = inv(K0) - K0iX @ solve(self.X.T @ K0iX, K0iX.T)

        # Pâ‚€ð² = Kâ»Â¹ð² - Kâ‚€â»Â¹X(Xáµ€Kâ‚€â»Â¹X)â»Â¹Xáµ€Kâ‚€â»Â¹ð².
        K0iy = solve(K0, self.y)
        P0y = K0iy - solve(K0, self.X @ solve(self.X.T @ K0iX, self.X.T @ K0iy))

        # The covariance matrix of H1 is K = Kâ‚€ + bÂ²diag(ð )â‹…Î£â‹…diag(ð )
        # We have âˆ‚K/âˆ‚bÂ² = diag(ð )â‹…Î£â‹…diag(ð )
        # The score test statistics is given by
        # Q = Â½ð²áµ€Pâ‚€â‹…âˆ‚Kâ‹…Pâ‚€ð²
        dK = ddot(g, ddot(self.Sigma, g))
        Q = (P0y.T @ dK @ P0y) / 2

        # Q is the score statistic for our interaction test and follows a linear combination
        # of chi-squared (df=1) distributions:
        # Q âˆ¼ âˆ‘Î»Ï‡Â², where Î»áµ¢ are the non-zero eigenvalues of Â½âˆšPâ‚€â‹…âˆ‚Kâ‹…âˆšPâ‚€.
        sqrP0 = sqrtm(P0)
        # lambdas = eigvalsh((sqrP0 @ dK @ sqrP0) / 2)
        # lambdas = lambdas[lambdas > epsilon.small]
        # print(lambdas)
        # print(Q)
        pval = davies_pvalue(Q, (sqrP0 @ dK @ sqrP0) / 2)
        return(pval)





